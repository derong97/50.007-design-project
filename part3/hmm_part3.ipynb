{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T15:07:02.336060Z",
     "start_time": "2020-11-19T15:07:02.229935Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions\n",
    "This is the fixed emission code from part 2. By \"fixed\", we mean that it takes into consideration unknown words not in train set (i.e. replaced with `\"#UNK#\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T12:49:06.715075Z",
     "start_time": "2020-11-19T12:49:06.696076Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_emission(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing emission parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state y, keep track of each observation count i.e. count (y -> x)\n",
    "    # before eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4}}\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # update emission_dict for state with count(y -> x) = 0\n",
    "    # after eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4, obs2: 0}}\n",
    "    observations = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        # process only valid lines\n",
    "        if len(split_line) == 2:\n",
    "            obs, state = split_line[0], split_line[1]\n",
    "            \n",
    "            observations.add(obs)\n",
    "            \n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "                \n",
    "            if obs not in emission_dict[state]:\n",
    "                emission_dict[state][obs] = 1\n",
    "            else:\n",
    "                emission_dict[state][obs] += 1\n",
    "\n",
    "    for k, v in emission_dict.items():\n",
    "        for obs in observations:\n",
    "            if obs not in v:\n",
    "                emission_dict[k][obs] = 0\n",
    "    \n",
    "    return emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T12:49:07.940349Z",
     "start_time": "2020-11-19T12:49:07.935354Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emission_params_fixed(emission_dict, state, obs, k=0.5):\n",
    "    \n",
    "    if state not in emission_dict:\n",
    "        raise Exception(\"State not in emission dict\")\n",
    "    \n",
    "    state_data = emission_dict[state]\n",
    "    count_y = sum(state_data.values()) # count(y)\n",
    "    \n",
    "    if obs == \"#UNK#\":\n",
    "        count_y_to_x = k\n",
    "    else:\n",
    "        count_y_to_x = state_data[obs] # count(y -> x)\n",
    "    \n",
    "    return count_y_to_x / (count_y + k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T12:54:56.727443Z",
     "start_time": "2020-11-19T12:54:56.705439Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_transition(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing transition parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state u, keep track of each state count i.e. count (u,v)\n",
    "    # before eg: {START: {y1: 1, y2: 5}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, STOP: 3}}\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # after eg: {START: {y1: 1, y2: 5, STOP: 0}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, y2: 0, STOP: 3}}\n",
    "    states = set()\n",
    "    states.add('STOP')\n",
    "    \n",
    "    prev_state = 'START'\n",
    "        \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        if prev_state not in transition_dict:\n",
    "            transition_dict[prev_state] = {}\n",
    "                \n",
    "        # can only be START or STOP\n",
    "        if len(split_line) < 2:\n",
    "            if 'STOP' not in transition_dict[prev_state]:\n",
    "                transition_dict[prev_state]['STOP'] = 0\n",
    "            \n",
    "            transition_dict[prev_state]['STOP'] += 1\n",
    "            prev_state = 'START'\n",
    "        \n",
    "        # processing the sentence\n",
    "        elif len(split_line) == 2:\n",
    "            curr_state = split_line[1]\n",
    "            states.add(curr_state)\n",
    "           \n",
    "            if curr_state not in transition_dict[prev_state]:\n",
    "                transition_dict[prev_state][curr_state] = 0\n",
    "            \n",
    "            transition_dict[prev_state][curr_state] += 1\n",
    "            prev_state = curr_state\n",
    "    \n",
    "    for k, v in transition_dict.items():\n",
    "        for state in states:\n",
    "            if state not in v:\n",
    "                transition_dict[k][state] = 0\n",
    "    \n",
    "    return transition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T12:54:57.364312Z",
     "start_time": "2020-11-19T12:54:57.351440Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_transition_params(transition_dict, u, v):\n",
    "    \n",
    "    if u not in transition_dict:\n",
    "        raise Exception(\"State u not in transition dict\")\n",
    "        \n",
    "    if v not in transition_dict[u]:\n",
    "        raise Exception(\"State v not in transition dict\")\n",
    "    \n",
    "    state_data = transition_dict[u]\n",
    "    \n",
    "    count_u_to_v = state_data[v] # count(u,v)\n",
    "    count_u = sum(state_data.values()) # count(u)\n",
    "            \n",
    "    return count_u_to_v / count_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T12:55:00.406339Z",
     "start_time": "2020-11-19T12:55:00.151320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1087041628604985"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_dict = train_transition('../dataset/EN/train')\n",
    "\n",
    "#transition_dict\n",
    "get_transition_params(transition_dict, 'START', 'B-PP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T13:27:18.944642Z",
     "start_time": "2020-11-19T13:27:18.924642Z"
    }
   },
   "outputs": [],
   "source": [
    "def obtain_all_obs(emission_dict):\n",
    "    \"\"\"\n",
    "    Obtain all distinct observations words in the emission_dict.\n",
    "    Purpose: This helps us identify words in Test Set that do not exist in the Training Set (or the emission_dict)\n",
    "    Returns - Set of Strings.\n",
    "    \"\"\"\n",
    "    all_observations = set()\n",
    "    \n",
    "    for s_to_obs_dict in emission_dict.values():\n",
    "        for obs in s_to_obs_dict.keys():\n",
    "            all_observations.add(obs)\n",
    "            \n",
    "    return all_observations\n",
    "\n",
    "def preprocess_sentence(sentence, training_set_words):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (words or observations)\n",
    "    Returns - a list of Strings, where Strings not in training_set_words are replaced by \"#UNK#\"\n",
    "    \"\"\"\n",
    "    return [ word if word in training_set_words else \"#UNK#\" for word in sentence ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T13:17:58.298665Z",
     "start_time": "2020-11-19T13:17:58.284664Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(filename):\n",
    "    \"\"\"\n",
    "    Returns - A 2-tuple (Dict, Dict): emission and transition parameters\n",
    "    \"\"\"\n",
    "    return train_emission(filename), train_transition(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T15:05:13.228007Z",
     "start_time": "2020-11-19T15:05:13.212009Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(emission_dict, transition_dict, sentence, is_preprocessed):\n",
    "    \"\"\"\n",
    "    Dynamic Programming approach (Viterbi algorithm) to generate state sequence given a sentence.\n",
    "    emission_dict - Dictionary. Emission parameters generated from training data.\n",
    "    transiiton_dict - Dictionary. Transition parameters generated from training data.\n",
    "    sentence - list of Strings (input words or observations)\n",
    "    is_preprocessed - boolean. True if variable sentence is preprocessed (unknown words (not in train) changed to \"#UNK#\")\n",
    "    Returns - 3-tuple (DataFrame, DataFrame, List of Strings)\n",
    "        Dataframe - Pi Table, |T| by n+2 dimension.\n",
    "        Dataframe - Backtrace Table, |T| by n+2 dimension.\n",
    "        List of Strings - (Predicted sequence of states corresponding to sentence).\n",
    "    \"\"\"\n",
    "    \n",
    "    all_states = list(emission_dict.keys())\n",
    "    \n",
    "    proc_sent = sentence\n",
    "    if not is_preprocessed:\n",
    "        training_set_words = obtain_all_obs(emission_dict)\n",
    "        proc_sent = preprocess_sentence(sentence, training_set_words)\n",
    "    proc_sent = [\"start\"] + proc_sent + [\"stop\"]\n",
    "    \n",
    "    n = len(proc_sent)\n",
    "\n",
    "    # Pi Table\n",
    "    P = pd.DataFrame(index=all_states + [\"START\", \"STOP\"], columns=range(n)).fillna(0)\n",
    "    # Backtrace Table\n",
    "    B = pd.DataFrame(index=all_states + [\"START\", \"STOP\"], columns=range(n))\n",
    "\n",
    "    # Base Case\n",
    "    P.loc['START', 0] = 1\n",
    "    \n",
    "    # Helper functions for recursive step\n",
    "    a = lambda u, v: get_transition_params(transition_dict, u, v)\n",
    "    b = lambda state, obs: get_emission_params_fixed(emission_dict, state, obs, k=0.5)\n",
    "\n",
    "    # Recursive Forward Step\n",
    "    for j in range(1, n-1): # Going right the columns (obs)\n",
    "        x = proc_sent[j]  # Obtain j'th word in the (processed) sentence\n",
    "\n",
    "        for v in all_states: # Going down the rows (states)\n",
    "            for u in all_states + [\"START\"]:\n",
    "                p = P.loc[u, j-1] * a(u, v) * b(v, x)\n",
    "                if p > P.loc[v, j]:\n",
    "                    P.loc[v, j] = p  # update probability\n",
    "                    B.loc[v, j] = u  # update backpointer\n",
    "\n",
    "    # Termination\n",
    "    j = n - 1\n",
    "    v = 'STOP'\n",
    "    for u in all_states:\n",
    "        p = P.loc[u, j-1] * a(u, v)\n",
    "        if p > P.loc[v, j]:\n",
    "            P.loc[v, j] = p  # probability\n",
    "            B.loc[v, j] = u  # backpointer\n",
    "\n",
    "    # Backtrace\n",
    "    state_seq = ['STOP']\n",
    "    for i in range(n-1, 0, -1):\n",
    "        curr_state = state_seq[-1]\n",
    "        prev_state = B.loc[curr_state, i]\n",
    "\n",
    "        if pd.isnull(prev_state):  # No possible transition to STOP. Edge case.\n",
    "            state_seq = ['O'] * n\n",
    "            break\n",
    "\n",
    "        state_seq.append(prev_state)\n",
    "    state_seq = state_seq[::-1][1:-1]  # reverse and drop START, STOP\n",
    "    \n",
    "    return P, B, state_seq  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:59:33.993271Z",
     "start_time": "2020-11-19T14:59:31.889767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-NP', 'B-VP', 'B-SBAR', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'B-ADJP', 'I-ADJP', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "emission_dict, transition_dict = train(filename)\n",
    "\n",
    "sentence = \"He added that the stress-related compensation claims is about twice the average for all injury claims .\"\n",
    "sentence = sentence.split(' ')\n",
    "\n",
    "_, _, seq = viterbi(emission_dict, transition_dict, sentence, is_preprocessed=False)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaton on dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T15:13:39.057469Z",
     "start_time": "2020-11-19T15:13:09.980767Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88783d5ed3b40e2ae909e859503c4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on EN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66acdabf5ca84c5f891c5f030afea922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-8be6efa32191>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0msent_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m# obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_state_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_proc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_preprocessed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_state_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-163-235a73abb4a1>\u001b[0m in \u001b[0;36mviterbi\u001b[1;34m(emission_dict, transition_dict, sentence, is_preprocessed)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Going down the rows (states)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"START\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m  \u001b[1;31m# update probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-163-235a73abb4a1>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(state, obs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Helper functions for recursive step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_transition_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_emission_params_fixed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Recursive Forward Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-86025f9360bd>\u001b[0m in \u001b[0;36mget_emission_params_fixed\u001b[1;34m(emission_dict, state, obs, k)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcount_y_to_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcount_y_to_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# count(y -> x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcount_y_to_x\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sets = ['EN', 'SG', 'CN']\n",
    "sets = [sets[0]]\n",
    "\n",
    "for dataset in tqdm(sets):\n",
    "    \n",
    "    print(f\"Evaluating on {dataset}.\")\n",
    "    \n",
    "    in_file = f\"../dataset/{dataset}/dev.in\"\n",
    "    train_file = f\"../dataset/{dataset}/train\"\n",
    "    out_file = f\"../dataset/{dataset}/dev.p3.out\"\n",
    "    \n",
    "    # Train\n",
    "    emission_dict, transition_dict = train(train_file)\n",
    "    # Obtain all distinct words in Training Set\n",
    "    training_set_words = obtain_all_obs(emission_dict)\n",
    "    \n",
    "    # Create file handler to write to /dev.p2.out\n",
    "    outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # Read in file\n",
    "    with open(in_file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sent = [] # initialise array to store 1 sentence at a time.\n",
    "    for word in tqdm(lines):\n",
    "        \n",
    "        if word != \"\\n\":\n",
    "            sent.append(word.strip())\n",
    "            \n",
    "        # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "        else:\n",
    "            # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "            sent_proc = preprocess_sentence(sent, training_set_words)\n",
    "            # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "            _, _, sent_state_sequence = viterbi(emission_dict, transition_dict, sent_proc, is_preprocessed=True)\n",
    "            \n",
    "            for word, state in zip(sent, sent_state_sequence):\n",
    "                outf_h.write(word + ' ' + state)\n",
    "                outf_h.write(\"\\n\") # newline for each word\n",
    "            outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "            # Reset sentence list\n",
    "            sent = []\n",
    "            \n",
    "    outf_h.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:13:10.330251Z",
     "start_time": "2020-11-19T14:13:09.888454Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = '../dataset/EN/train'\n",
    "emission_dict, transition_dict = train(filename)\n",
    "\n",
    "training_set_words = obtain_all_obs(emission_dict)\n",
    "all_states = list(emission_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:13:10.346250Z",
     "start_time": "2020-11-19T14:13:10.332249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "['The', 'quick', '#UNK#', 'fox', 'jumps', 'over', 'the', '#UNK#', 'dog', '.']\n",
      "['start', 'The', 'quick', '#UNK#', 'fox', 'jumps', 'over', 'the', '#UNK#', 'dog', '.', 'stop']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog .\"\n",
    "sentence = sentence.split(' ')\n",
    "proc_sent = preprocess_sentence(sentence, training_set_words)\n",
    "print(sentence)\n",
    "print(proc_sent)\n",
    "\n",
    "proc_sent = [\"start\"] + proc_sent + [\"stop\"]\n",
    "print(proc_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:42:41.823499Z",
     "start_time": "2020-11-19T14:42:41.801498Z"
    }
   },
   "outputs": [],
   "source": [
    "n = len(proc_sent)\n",
    "\n",
    "# Pi Table\n",
    "P = pd.DataFrame(index=all_states + [\"START\", \"STOP\"], columns=range(n)).fillna(0)\n",
    "\n",
    "# Backpropagate\n",
    "B = pd.DataFrame(index=all_states + [\"START\", \"STOP\"], columns=range(n))\n",
    "\n",
    "# Base Case\n",
    "P.loc['START', 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:42:42.977658Z",
     "start_time": "2020-11-19T14:42:41.986511Z"
    }
   },
   "outputs": [],
   "source": [
    "a = lambda u, v: get_transition_params(transition_dict, u, v)\n",
    "b = lambda state, obs: get_emission_params_fixed(emission_dict, state, obs, k=0.5)\n",
    "\n",
    "# Recursive Forward Step\n",
    "for j in range(1, n-1): # Going right the columns (obs)\n",
    "    x = proc_sent[j]  # Obtain j'th word in the (processed) sentence\n",
    "\n",
    "    for v in all_states: # Going down the rows (states)\n",
    "        for u in all_states + [\"START\"]:\n",
    "            p = P.loc[u, j-1] * a(u, v) * b(v, x)\n",
    "            if p > P.loc[v, j]:\n",
    "                P.loc[v, j] = p  # update probability\n",
    "                B.loc[v, j] = u  # update backpointer\n",
    "    \n",
    "# termination\n",
    "j = n - 1\n",
    "v = 'STOP'\n",
    "for u in all_states:\n",
    "    p = P.loc[u, j-1] * a(u, v)\n",
    "    if p > P.loc[v, j]:\n",
    "        P.loc[v, j] = p  # probability\n",
    "        B.loc[v, j] = u  # backpointer\n",
    "        \n",
    "# backtrace\n",
    "state_seq = ['STOP']\n",
    "for i in range(n-1, 0, -1):\n",
    "    curr_state = state_seq[-1]\n",
    "    prev_state = B.loc[curr_state, i]\n",
    "    \n",
    "    if pd.isnull(prev_state):  # No possible transition to STOP. Edge case.\n",
    "        state_seq = ['O'] * n\n",
    "        break\n",
    "        \n",
    "    state_seq.append(prev_state)\n",
    "state_seq = state_seq[::-1][1:-1]  # reverse and drop START, STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:43:02.005216Z",
     "start_time": "2020-11-19T14:43:01.993215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "['B-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(state_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
