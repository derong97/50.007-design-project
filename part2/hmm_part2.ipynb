{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:49.331423Z",
     "start_time": "2020-11-18T14:51:49.262234Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:50.207277Z",
     "start_time": "2020-11-18T14:51:50.190278Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_emission(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing emission parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state y, keep track of each observation count i.e. count (y -> x)\n",
    "    # before eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4}}\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # update emission_dict for state with count(y -> x) = 0\n",
    "    # after eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4, obs2: 0}}\n",
    "    observations = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        # process only valid lines\n",
    "        if len(split_line) == 2:\n",
    "            obs, state = split_line[0], split_line[1]\n",
    "            \n",
    "            observations.add(obs)\n",
    "            \n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "                \n",
    "            if obs not in emission_dict[state]:\n",
    "                emission_dict[state][obs] = 1\n",
    "            else:\n",
    "                emission_dict[state][obs] += 1\n",
    "\n",
    "    for k, v in emission_dict.items():\n",
    "        for obs in observations:\n",
    "            if obs not in v:\n",
    "                emission_dict[k][obs] = 0\n",
    "    \n",
    "    return emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:50.977230Z",
     "start_time": "2020-11-18T14:51:50.965230Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emission_params(emission_dict, state, obs):\n",
    "    \n",
    "    if state not in emission_dict:\n",
    "        raise Exception(\"State not in emission dict\")\n",
    "    \n",
    "    state_data = emission_dict[state]\n",
    "    \n",
    "    if obs not in state_data:\n",
    "        raise Exception(\"Word did not appear in training data\")\n",
    "    \n",
    "    count_y_to_x = state_data[obs] # count(y -> x)\n",
    "    count_y = sum(state_data.values()) # count(y)\n",
    "    \n",
    "    return count_y_to_x / count_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:51.789247Z",
     "start_time": "2020-11-18T14:51:51.549231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003846787932076716"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict = train_emission('../dataset/EN/train')\n",
    "\n",
    "# emission_dict\n",
    "get_emission_params(emission_dict, 'I-NP', 'corporate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the computation of emission probabilities\n",
    "This is to account for words that appear in the test set but do not appear in the training set. Before running this function, such words should have been replaced by the `#UNK#` token during the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:53.968248Z",
     "start_time": "2020-11-18T14:51:53.951250Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emission_params_fixed(emission_dict, state, obs, k=0.5):\n",
    "    \n",
    "    if state not in emission_dict:\n",
    "        raise Exception(\"State not in emission dict\")\n",
    "    \n",
    "    state_data = emission_dict[state]\n",
    "    count_y = sum(state_data.values()) # count(y)\n",
    "    \n",
    "    if obs == \"#UNK#\":\n",
    "        count_y_to_x = k\n",
    "    else:\n",
    "        count_y_to_x = state_data[obs] # count(y -> x)\n",
    "    \n",
    "    return count_y_to_x / (count_y + k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a simple system that produces the tag for each word `x` in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:54.846260Z",
     "start_time": "2020-11-18T14:51:54.837260Z"
    }
   },
   "outputs": [],
   "source": [
    "def obtain_all_obs(emission_dict):\n",
    "    \"\"\"\n",
    "    Obtain all distinct observations words in the emission_dict.\n",
    "    Purpose: This helps us identify words in Test Set that do not exist in the Training Set (or the emission_dict)\n",
    "    Returns - Set of Strings.\n",
    "    \"\"\"\n",
    "    all_observations = set()\n",
    "    \n",
    "    for s_to_obs_dict in emission_dict.values():\n",
    "        for obs in s_to_obs_dict.keys():\n",
    "            all_observations.add(obs)\n",
    "            \n",
    "    return all_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:55.563229Z",
     "start_time": "2020-11-18T14:51:55.549229Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, training_set_words):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (word or observations)\n",
    "    Returns - a list of Strings, where Strings not in training_set_words are replaced by \"#UNK#\"\n",
    "    \"\"\"\n",
    "    return [ word if word in training_set_words else \"#UNK#\" for word in sentence ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:55.781233Z",
     "start_time": "2020-11-18T14:51:55.773232Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_sequence(sentence, emission_dict):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (words or observations).\n",
    "    emission_dict - a dictionary containing emission parameters\n",
    "    Returns - list of Strings (corresponding highest prob state for each word)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_states = list(emission_dict.keys()) # all distinct states\n",
    "    \n",
    "    sequence = [] # aka tags\n",
    "    \n",
    "    for word in sentence:\n",
    "        emission_state = { state: get_emission_params_fixed(emission_dict, state, word) for state in all_states }\n",
    "        sequence.append(max(emission_state, key=lambda state: emission_state[state]))\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:51:56.765236Z",
     "start_time": "2020-11-18T14:51:56.669234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', '#UNK#', 'fox', 'jumps', 'over', 'the', '#UNK#', 'dog', '.']\n",
      "['B-NP', 'B-NP', 'B-UCP', 'B-NP', 'I-NP', 'B-PRT', 'B-NP', 'B-UCP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "training_set_words = obtain_all_obs(emission_dict)\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog .\"\n",
    "# sentence = \"The incident occurred Saturday night .\"\n",
    "\n",
    "sentence = sentence.split(' ')\n",
    "sentence = preprocess_sentence(sentence, training_set_words)\n",
    "\n",
    "print(sentence)\n",
    "print(label_sequence(sentence, emission_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:55:51.862537Z",
     "start_time": "2020-11-18T14:52:10.269238Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f770bbb173d4465b475b550529155e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on EN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19f55397f644f9db9005974e8303624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on SG.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537f593988934a2f8463b55cb5d2a4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36841.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on CN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0a247f9f7f4037914cf5eb6e388ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sets = ['EN', 'SG', 'CN']\n",
    "\n",
    "for dataset in tqdm(sets):\n",
    "    \n",
    "    print(f\"Evaluating on {dataset}.\")\n",
    "    \n",
    "    in_file = f\"../dataset/{dataset}/dev.in\"\n",
    "    train_file = f\"../dataset/{dataset}/train\"\n",
    "    out_file = f\"../dataset/{dataset}/dev.p2.out\"\n",
    "    \n",
    "    # Train\n",
    "    emission_dict = train_emission(train_file)\n",
    "    # Obtain all distinct words in Training Set\n",
    "    training_set_words = obtain_all_obs(emission_dict)\n",
    "    \n",
    "    # Create file handler to write to /dev.p2.out\n",
    "    outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # Read in file\n",
    "    with open(in_file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sent = [] # initialise array to store 1 sentence at a time.\n",
    "    for word in tqdm(lines):\n",
    "        \n",
    "        if word != \"\\n\":\n",
    "            sent.append(word.strip())\n",
    "            \n",
    "        # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "        else:\n",
    "            # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "            sent_proc = preprocess_sentence(sent, training_set_words)\n",
    "            # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "            sent_state_sequence = label_sequence(sent_proc, emission_dict)\n",
    "\n",
    "            for word, state in zip(sent, sent_state_sequence):\n",
    "                outf_h.write(word + ' ' + state)\n",
    "                outf_h.write(\"\\n\") # newline for each word\n",
    "            outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "            # Reset sentence list\n",
    "            sent = []\n",
    "    \n",
    "    outf_h.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:45:12.729170Z",
     "start_time": "2020-11-18T14:45:12.653555Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set_words = obtain_all_obs(emission_dict)\n",
    "\n",
    "with open(\"../dataset/SG/dev.in\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:48:06.988083Z",
     "start_time": "2020-11-18T14:48:01.469667Z"
    }
   },
   "outputs": [],
   "source": [
    "sent = []\n",
    "\n",
    "outf = open(\"demo.txt\", \"w\", encoding=\"utf8\")\n",
    "for word in tqdm(lines):\n",
    "\n",
    "    if word != \"\\n\":\n",
    "        sent.append(word.strip())\n",
    "    \n",
    "    # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "    else: \n",
    "        # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "        sent_proc = preprocess_sentence(sent, training_set_words)\n",
    "        # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "        sent_state_sequence = label_sequence(sent_proc, emission_dict)\n",
    "\n",
    "        for word, state in zip(sent, sent_state_sequence):\n",
    "            outf.write(word + ' ' + state)\n",
    "            outf.write(\"\\n\") # newline for each word\n",
    "        outf.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "        # Reset sentence list\n",
    "        sent = []\n",
    "\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T15:03:34.438409Z",
     "start_time": "2020-11-18T15:03:34.429410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deron\\Desktop\\50.007-design-project\\dataset\\EvalScript\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\deron\\\\Desktop\\\\50.007-design-project\\\\dataset\\\\EvalScript'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../dataset/EvalScript\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T15:07:08.009043Z",
     "start_time": "2020-11-18T15:07:07.381468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 18650\n",
      "\n",
      "#Correct Entity : 9542\n",
      "Entity  precision: 0.5116\n",
      "Entity  recall: 0.7240\n",
      "Entity  F: 0.5996\n",
      "\n",
      "#Correct Sentiment : 8456\n",
      "Sentiment  precision: 0.4534\n",
      "Sentiment  recall: 0.6416\n",
      "Sentiment  F: 0.5313\n",
      "====================\n",
      "\n",
      "SG\n",
      "\n",
      "#Entity in gold data: 4301\n",
      "#Entity in prediction: 12237\n",
      "\n",
      "#Correct Entity : 2386====================\n",
      "Entity  precision: 0.1950\n",
      "Entity  recall: 0.5548\n",
      "Entity  F: 0.2885\n",
      "\n",
      "\n",
      "CN\n",
      "\n",
      "#Correct Sentiment : 1531\n",
      "Sentiment  precision: 0.1251\n",
      "Sentiment  recall: 0.3560\n",
      "Sentiment  F: 0.1851\n",
      "\n",
      "#Entity in gold data: 700\n",
      "#Entity in prediction: 4248\n",
      "====================\n",
      "#Correct Entity : 345\n",
      "Entity  precision: 0.0812\n",
      "Entity  recall: 0.4929\n",
      "Entity  F: 0.1395\n",
      "\n",
      "#Correct Sentiment : 167\n",
      "Sentiment  precision: 0.0393\n",
      "Sentiment  recall: 0.2386\n",
      "Sentiment  F: 0.0675\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for set in [\"EN\", \"SG\", \"CN\"]:\n",
    "    gold = f\"../{set}/dev.out\"\n",
    "    pred = f\"../{set}/dev.p2.out\"\n",
    "    print(set)\n",
    "    !python evalResult.py $gold $pred\n",
    "    print(\"=\" * 20, end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
