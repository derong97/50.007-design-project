{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:28.721852Z",
     "start_time": "2020-12-05T16:24:28.717851Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emission\n",
    "Copied from part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:29.615148Z",
     "start_time": "2020-12-05T16:24:29.605146Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_emission(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing emission parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state y, keep track of each observation count i.e. count (y -> x)\n",
    "    # before eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4}}\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # update emission_dict for state with count(y -> x) = 0\n",
    "    # after eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4, obs2: 0}}\n",
    "    observations = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        # process only valid lines\n",
    "        if len(split_line) == 2:\n",
    "            obs, state = split_line[0], split_line[1]\n",
    "            \n",
    "            observations.add(obs)\n",
    "            \n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "                \n",
    "            if obs not in emission_dict[state]:\n",
    "                emission_dict[state][obs] = 1\n",
    "            else:\n",
    "                emission_dict[state][obs] += 1\n",
    "\n",
    "    for k, v in emission_dict.items():\n",
    "        for obs in observations:\n",
    "            if obs not in v:\n",
    "                emission_dict[k][obs] = 0\n",
    "    \n",
    "    return emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:30.276850Z",
     "start_time": "2020-12-05T16:24:30.268850Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emission_params_fixed(emission_dict, state, obs, k=0.5):\n",
    "    \n",
    "    if state not in emission_dict:\n",
    "        raise Exception(\"State not in emission dict\")\n",
    "    \n",
    "    state_data = emission_dict[state]\n",
    "    count_y = sum(state_data.values()) # count(y)\n",
    "    \n",
    "    if obs == \"#UNK#\":\n",
    "        count_y_to_x = k\n",
    "    else:\n",
    "        count_y_to_x = state_data[obs] # count(y -> x)\n",
    "    \n",
    "    return count_y_to_x / (count_y + k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitions\n",
    "Copied from part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:31.078853Z",
     "start_time": "2020-12-05T16:24:31.060852Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_transition(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing transition parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state u, keep track of each state count i.e. count (u,v)\n",
    "    # before eg: {START: {y1: 1, y2: 5}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, STOP: 3}}\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # after eg: {START: {y1: 1, y2: 5, STOP: 0}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, y2: 0, STOP: 3}}\n",
    "    states = set()\n",
    "    states.add('STOP')\n",
    "    \n",
    "    prev_state = 'START'\n",
    "        \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        if prev_state not in transition_dict:\n",
    "            transition_dict[prev_state] = {}\n",
    "                \n",
    "        # can only be START or STOP\n",
    "        if len(split_line) < 2:\n",
    "            if 'STOP' not in transition_dict[prev_state]:\n",
    "                transition_dict[prev_state]['STOP'] = 0\n",
    "            \n",
    "            transition_dict[prev_state]['STOP'] += 1\n",
    "            prev_state = 'START'\n",
    "        \n",
    "        # processing the sentence\n",
    "        elif len(split_line) == 2:\n",
    "            curr_state = split_line[1]\n",
    "            states.add(curr_state)\n",
    "           \n",
    "            if curr_state not in transition_dict[prev_state]:\n",
    "                transition_dict[prev_state][curr_state] = 0\n",
    "            \n",
    "            transition_dict[prev_state][curr_state] += 1\n",
    "            prev_state = curr_state\n",
    "    \n",
    "    for k, v in transition_dict.items():\n",
    "        for state in states:\n",
    "            if state not in v:\n",
    "                transition_dict[k][state] = 0\n",
    "    \n",
    "    return transition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:31.685851Z",
     "start_time": "2020-12-05T16:24:31.676850Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_transition_params(transition_dict, u, v):\n",
    "    \n",
    "    if u not in transition_dict:\n",
    "        raise Exception(\"State u not in transition dict\")\n",
    "        \n",
    "    if v not in transition_dict[u]:\n",
    "        raise Exception(\"State v not in transition dict\")\n",
    "    \n",
    "    state_data = transition_dict[u]\n",
    "    \n",
    "    count_u_to_v = state_data[v] # count(u,v)\n",
    "    count_u = sum(state_data.values()) # count(u)\n",
    "            \n",
    "    return count_u_to_v / count_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Copied from part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(m):\n",
    "    if isinstance(m, float) or isinstance(m, int):\n",
    "        return -np.inf if m == 0 else np.log(m)\n",
    "    \n",
    "    m = np.clip(m, 1e-32, None)\n",
    "    x = np.log(m)\n",
    "    \n",
    "    x[x <= np.log(1e-32)] = -np.inf\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:32.141863Z",
     "start_time": "2020-12-05T16:24:32.131863Z"
    }
   },
   "outputs": [],
   "source": [
    "def obtain_all_obs(emission_dict):\n",
    "    \"\"\"\n",
    "    Obtain all distinct observations words in the emission_dict.\n",
    "    Purpose: This helps us identify words in Test Set that do not exist in the Training Set (or the emission_dict)\n",
    "    Returns - Set of Strings.\n",
    "    \"\"\"\n",
    "    all_observations = set()\n",
    "    \n",
    "    for s_to_obs_dict in emission_dict.values():\n",
    "        for obs in s_to_obs_dict.keys():\n",
    "            all_observations.add(obs)\n",
    "            \n",
    "    return all_observations\n",
    "\n",
    "def preprocess_sentence(sentence, training_set_words):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (words or observations)\n",
    "    Returns - a list of Strings, where Strings not in training_set_words are replaced by \"#UNK#\"\n",
    "    \"\"\"\n",
    "    return [ word if word in training_set_words else \"#UNK#\" for word in sentence ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:32.925847Z",
     "start_time": "2020-12-05T16:24:32.915846Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(filename):\n",
    "    \"\"\"\n",
    "    Returns - A 2-tuple (Dict, Dict): emission and transition parameters\n",
    "    \"\"\"\n",
    "    return train_emission(filename), train_transition(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:33.839272Z",
     "start_time": "2020-12-05T16:24:33.781136Z"
    }
   },
   "outputs": [],
   "source": [
    "def topk_viterbi(emission_dict, transition_dict, sentence, is_preprocessed, k=3):\n",
    "    all_states = list(emission_dict.keys())\n",
    "    \n",
    "    proc_sent = sentence\n",
    "    if not is_preprocessed:\n",
    "        training_set_words = obtain_all_obs(emission_dict)\n",
    "        proc_sent = preprocess_sentence(sentence, training_set_words)\n",
    "    proc_sent = [\"start\"] + proc_sent + [\"stop\"]\n",
    "    \n",
    "    n = len(proc_sent)\n",
    "    \n",
    "    # Pi Table\n",
    "    P = np.ones( (len(proc_sent), len(all_states), k) ) * -np.inf\n",
    "    \n",
    "    # Backtrace Table\n",
    "    B = [ [ [None] * k for x in all_states ] for y in proc_sent ]\n",
    "    \n",
    "    a = lambda u, v: get_transition_params(transition_dict, u, v)\n",
    "    b = lambda state, obs: get_emission_params_fixed(emission_dict, state, obs, k=0.5)\n",
    "\n",
    "    # Base Case at j=1\n",
    "    t = np.array([ a('START', v) for v in all_states ])\n",
    "    e = np.array([ b(v, proc_sent[1]) for v in all_states ])\n",
    "    P[1, :, 0] = log(t) + log(e)\n",
    "\n",
    "    B[1] = [ [\"START\"] + [None] * (k-1) for row in B[1] ]\n",
    "    \n",
    "    # Recursive Forward Step for j=2 to n-2 (inclusive). Where n = len(proc_sent) => Layer n is the last layer (STOP).\n",
    "    # At layer n-1, we perform termination step, as layer n is STOP. \n",
    "    # Recall that proc_sent includes 'start' and 'stop'\n",
    "\n",
    "    for j in range(2, n-1): # Going right the columns (obs)\n",
    "        x = proc_sent[j]  # Obtain j'th word in the (processed) sentence\n",
    "\n",
    "        for row_no, v in enumerate(all_states): # Going down the rows (states)\n",
    "            transitions = np.array([ a(u, v) for u in all_states ])\n",
    "            previous_all_scores = (P[j-1, :] + log(transitions[:, None])).flatten()\n",
    "            topk = previous_all_scores.argsort()[::-1][:k]\n",
    "            P[j,row_no] = previous_all_scores[topk] + log(b(v,x))\n",
    "            B[j][row_no] = [ all_states[pos // k] for pos in topk ]\n",
    "            \n",
    "            for i, sub_k in enumerate(P[j,row_no]):\n",
    "                if sub_k == -np.inf:\n",
    "                    B[j][row_no][i] = None\n",
    "            \n",
    "    # Termination: j=n-1. Note that proc_sent[n-1] give us the last word in sentence.\n",
    "    j = n-1\n",
    "    transitions = np.array([ a(u, \"STOP\") for u in all_states ])\n",
    "    previous_all_scores = (P[j-1] + log(transitions[:, None])).flatten()\n",
    "    final_topk = previous_all_scores.argsort()[::-1][:k]\n",
    "    final_scores = previous_all_scores[final_topk]\n",
    "    \n",
    "    # top k parent STATES preceding the STOP state. By top k, means top k best scores.\n",
    "    final_topk_pos = [ all_states[pos // k] for pos in final_topk ]\n",
    "    \n",
    "    # Backtrace\n",
    "    state_seq = ['STOP']\n",
    "    \n",
    "    prev_states = final_topk_pos\n",
    "    prev_state = prev_states[-1] # you already know you want the kth best, which is the last\n",
    "    prev_row_no = all_states.index(prev_state)\n",
    "    curr_score = final_scores[-1]\n",
    "    \n",
    "    # from n-2 to n-1 (STOP)\n",
    "    j = n-1\n",
    "    for i in range(k):\n",
    "        prev_score = P[j-1, prev_row_no][i]\n",
    "        if prev_score + log(a(prev_state, \"STOP\")) == curr_score:\n",
    "            curr_score = prev_score\n",
    "            curr_idx = i\n",
    "            state_seq.append(prev_state)\n",
    "            break\n",
    "    \n",
    "    for j in range(n-2, 1, -1):\n",
    "        x = proc_sent[j]\n",
    "        curr_state = state_seq[-1]\n",
    "        curr_row_no = all_states.index(curr_state)\n",
    "        prev_state = B[j][curr_row_no][curr_idx]\n",
    "        \n",
    "        if prev_state == None: # No possible transition to STOP. Edge case.\n",
    "            state_seq = ['O'] * (n-2)\n",
    "            return P, B, state_seq\n",
    "        \n",
    "        prev_row_no = all_states.index(prev_state)\n",
    "        \n",
    "        for i in range(k):\n",
    "            prev_score = P[j-1, prev_row_no][i]\n",
    "            if prev_score + log(a(prev_state, curr_state)) + log(b(curr_state, x)) == curr_score:\n",
    "                curr_score = prev_score\n",
    "                curr_idx = i\n",
    "                state_seq.append(prev_state)\n",
    "                break\n",
    "    \n",
    "    state_seq.append(\"START\") # START will throw an error because it has no index in all_states\n",
    "    state_seq = state_seq[::-1][1:-1]  # reverse and drop START, STOP\n",
    "    \n",
    "    return P, B, state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Example\n",
    "If you set k=1, the result should be the same as part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:24:35.843576Z",
     "start_time": "2020-12-05T16:24:35.317527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'B-SBAR', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict, transition_dict = train('../dataset/EN/train')\n",
    "training_set_words = obtain_all_obs(emission_dict)\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog .\"\n",
    "\n",
    "sentence = sentence.split(' ')\n",
    "proc_sent = preprocess_sentence(sentence, training_set_words)\n",
    "\n",
    "#topk_viterbi(emission_dict, transition_dict, proc_sent, is_preprocessed=True)\n",
    "P, B, seq = topk_viterbi(emission_dict, transition_dict, proc_sent, is_preprocessed=True, k=3)\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on EN/dev.in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:26:57.006328Z",
     "start_time": "2020-12-05T16:24:38.292368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on EN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f921befbc34eb780b2f90eb6191dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'EN'\n",
    "\n",
    "print(f\"Evaluating on {dataset}.\")\n",
    "\n",
    "in_file = f\"../dataset/{dataset}/dev.in\"\n",
    "train_file = f\"../dataset/{dataset}/train\"\n",
    "out_file = f\"../dataset/{dataset}/dev.p4.out\"\n",
    "\n",
    "# Train\n",
    "emission_dict, transition_dict = train(train_file)\n",
    "# Obtain all distinct words in Training Set\n",
    "training_set_words = obtain_all_obs(emission_dict)\n",
    "\n",
    "# Create file handler to write to /dev.p2.out\n",
    "outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "\n",
    "# Read in file\n",
    "with open(in_file, encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sent = [] # initialise array to store 1 sentence at a time.\n",
    "for word in tqdm(lines):\n",
    "\n",
    "    if word != \"\\n\":\n",
    "        sent.append(word.strip())\n",
    "\n",
    "    # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "    else:\n",
    "        # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "        sent_proc = preprocess_sentence(sent, training_set_words)\n",
    "        # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "        _, _, sent_state_sequence = topk_viterbi(emission_dict, transition_dict, sent_proc, is_preprocessed=True, k=3)\n",
    "\n",
    "        for word, state in zip(sent, sent_state_sequence):\n",
    "            outf_h.write(word + ' ' + state)\n",
    "            outf_h.write(\"\\n\") # newline for each word\n",
    "        outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "        # Reset sentence list\n",
    "        sent = []\n",
    "\n",
    "outf_h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running EvalScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:26:57.022334Z",
     "start_time": "2020-12-05T16:26:57.008328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deron\\Desktop\\50.007-design-project\\EvalScript\n"
     ]
    }
   ],
   "source": [
    "%cd ../EvalScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T16:26:57.844469Z",
     "start_time": "2020-12-05T16:26:57.025332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 12076\n",
      "\n",
      "#Correct Entity : 9333\n",
      "Entity  precision: 0.7729\n",
      "Entity  recall: 0.7082\n",
      "Entity  F: 0.7391\n",
      "\n",
      "#Correct Sentiment : 8849\n",
      "Sentiment  precision: 0.7328\n",
      "Sentiment  recall: 0.6714\n",
      "Sentiment  F: 0.7008\n"
     ]
    }
   ],
   "source": [
    "gold = f\"../dataset/{dataset}/dev.out\"\n",
    "pred = f\"../dataset/{dataset}/dev.p4.out\"\n",
    "\n",
    "!python evalResult.py $gold $pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
