{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transition(filename):\n",
    "    \"\"\"\n",
    "    Returns - dataframe with t, u as index and v as columns containing probability of t, u -> v\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Store all set of 3 state transitions\n",
    "    # eg: {(PREVSTART, START): {state1 : 1, ...}}\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # Store all unique states\n",
    "    states = set()\n",
    "    states.add('STOP')\n",
    "    states.add('PREVSTART')\n",
    "    states.add('START')\n",
    "    \n",
    "    prev_prev_state = 'PREVSTART'\n",
    "    prev_state = 'START'\n",
    "        \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "                \n",
    "        # Start new sequence\n",
    "        if len(split_line) < 2:\n",
    "            if (prev_prev_state, prev_state) not in transition_dict.keys():\n",
    "                transition_dict[(prev_prev_state, prev_state)] = defaultdict(int)\n",
    "            transition_dict[(prev_prev_state, prev_state)]['STOP'] += 1\n",
    "            prev_prev_state = 'PREVSTART'\n",
    "            prev_state = 'START'\n",
    "\n",
    "        # Processing the current sequence\n",
    "        elif len(split_line) == 2:\n",
    "            curr_state = split_line[1]\n",
    "            states.add(curr_state)\n",
    "            if (prev_prev_state, prev_state) not in transition_dict.keys():\n",
    "                transition_dict[(prev_prev_state, prev_state)] = defaultdict(int)\n",
    "            transition_dict[(prev_prev_state, prev_state)][curr_state] += 1\n",
    "            prev_prev_state = prev_state\n",
    "            prev_state = curr_state\n",
    "            \n",
    "    # Convert each count to a probability\n",
    "    for tu, vs in transition_dict.items():\n",
    "        count_tu = sum(vs.values())\n",
    "        for v in vs:\n",
    "            transition_dict[tu][v] = transition_dict[tu][v]/count_tu\n",
    "    \n",
    "    # Convert set of states to list \n",
    "    states = list(states)\n",
    "    \n",
    "    # Generate all possible pairings of states\n",
    "    all_state_pairs = []\n",
    "    for t in states:\n",
    "        for u in states:\n",
    "            all_state_pairs.append((t, u))\n",
    "    \n",
    "    # Create a numpy matrix to store all the transition probabilities\n",
    "    np_transition_matrix = np.zeros((len(all_state_pairs), len(states)))\n",
    "    \n",
    "    for i in range(len(all_state_pairs)):\n",
    "        for j in range(len(states)):\n",
    "            tu = all_state_pairs[i]\n",
    "            v = states[j]\n",
    "            if tu in transition_dict.keys():\n",
    "                np_transition_matrix[i][j] = transition_dict[tu[0], tu[1]][v]\n",
    "            else:\n",
    "                np_transition_matrix[i][j] = 0\n",
    "    \n",
    "    # Convert into DataFrame for easy indexing\n",
    "    df_transition_matrix = pd.DataFrame(np_transition_matrix, index = all_state_pairs, columns = states)\n",
    "    \n",
    "    return df_transition_matrix, all_state_pairs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emission(filename, states_w_start_stop, k = 0.5):\n",
    "    \"\"\"\n",
    "    Returns - dataframe with states as index and word as column, containing the probability of state -> word\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state y, keep track of each observation count i.e. count (y -> x)\n",
    "    # before eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4}}\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # update emission_dict for state with count(y -> x) = 0\n",
    "    # after eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4, obs2: 0}}\n",
    "    observations = set()\n",
    "    states = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        # process only valid lines\n",
    "        if len(split_line) == 2:\n",
    "            obs, state = split_line[0], split_line[1]\n",
    "            states.add(state)\n",
    "            observations.add(obs)\n",
    "            \n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "                \n",
    "            if obs not in emission_dict[state]:\n",
    "                emission_dict[state][obs] = 1\n",
    "            else:\n",
    "                emission_dict[state][obs] += 1\n",
    "\n",
    "    for key, value in emission_dict.items():\n",
    "        for obs in observations:\n",
    "            if obs not in value:\n",
    "                emission_dict[key][obs] = 0\n",
    "    \n",
    "    # Convert state and observation set to list\n",
    "    states = list(states)\n",
    "    observations = list(observations) + ['#UNK#'] # Add the #UNK# word into observations at the end\n",
    "    \n",
    "    # Create a numpy matrix to store all the emission probabilities\n",
    "    np_emission_matrix = np.zeros((len(states_w_start_stop), len(observations)))\n",
    "    for i in range(len(states_w_start_stop)):\n",
    "        state = states_w_start_stop[i]\n",
    "        \n",
    "        if state == 'PREVSTART' or state == 'START' or state == 'STOP': # These states don't have emissions\n",
    "            continue\n",
    "            \n",
    "        v_count = sum(emission_dict[state].values())\n",
    "        for j in range(len(observations) - 1):\n",
    "            state = states_w_start_stop[i]\n",
    "            obs = observations[j]\n",
    "            np_emission_matrix[i][j] = emission_dict[state][obs]/v_count # count(u -> v) / v_count\n",
    "        \n",
    "        # Add Laplace smoothing of k = 0.5 for #UNK# words\n",
    "        np_emission_matrix[i][len(observations) - 1] = k/(v_count + k)\n",
    "        \n",
    "    \n",
    "    # Convert to df for easy indexing\n",
    "    df_emission_matrix = pd.DataFrame(np_emission_matrix, index = states_w_start_stop, columns = observations)\n",
    "    \n",
    "    return df_emission_matrix, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, training_set_words):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (words or observations)\n",
    "    Returns - a list of Strings, where Strings not in training_set_words are replaced by \"#UNK#\"\n",
    "    \"\"\"\n",
    "    return [ word if word in training_set_words else \"#UNK#\" for word in sentence ]\n",
    "\n",
    "# Simple log function to prevent underflow\n",
    "def log(x):\n",
    "    x = np.clip(x, 1e-32, None)\n",
    "    return np.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_viterbi(df_emission_matrix, observations, states, df_transition_matrix, all_state_pairs, sentence, is_preprocessed):\n",
    "        \n",
    "    proc_sent = sentence\n",
    "    if not is_preprocessed:\n",
    "        proc_sent = preprocess_sentence(sentence, observations)\n",
    "    proc_sent = ['start'] + proc_sent + ['stop']\n",
    "    \n",
    "    # Add 2 for START and STOP states\n",
    "    n = len(proc_sent)\n",
    "    \n",
    "    # Pi Table\n",
    "    P = pd.DataFrame(index = all_state_pairs, columns = range(n)).fillna(-np.inf)\n",
    "    \n",
    "    # Backpointer Table\n",
    "    B = pd.DataFrame(index = all_state_pairs, columns = range(n))\n",
    "    \n",
    "    # Initialisation\n",
    "    P.loc[(\"PREVSTART\", \"START\"), 0] = 1\n",
    "    \n",
    "    # Forward Recursive Step\n",
    "    for j in range(1, n-1):\n",
    "        x = proc_sent[j]\n",
    "            \n",
    "        a_b = log(df_transition_matrix.multiply(df_emission_matrix[x], axis = 'columns')) # log(a(t,u,v) * b(v,x))\n",
    "        pis = a_b.add(P[j-1], axis = 'index').astype('float64') # log(a(t,u,v) * b(v,x)) + pi at j-1\n",
    "        \n",
    "        for curr_state in states:\n",
    "            if curr_state == 'STOP':\n",
    "                continue\n",
    "            tu = all_state_pairs[np.argmax(pis[curr_state].values)] # Get the highest arg for t, u -> v\n",
    "            score = np.max(pis[curr_state].values) # Get highest score\n",
    "            P.loc[[(tu[1], curr_state)], j] = score # Store score in Pi table at (u, v)\n",
    "            B.loc[[(tu[1], curr_state)], j] = tu[0] # Store t in Backtrace table at (u, v)\n",
    "    \n",
    "    # Termination\n",
    "    j = n-1\n",
    "    a_b = log(df_transition_matrix['STOP']).add(P[j-1],axis = 'index').astype('float64')\n",
    "    tu = a_b.idxmax() # t, u -> STOP\n",
    "    score = a_b.max()\n",
    "    P.loc[[(tu[1], 'STOP')], j] = score\n",
    "    B.loc[[(tu[1], 'STOP')], j] = tu[0]\n",
    "    \n",
    "    # Backtrack\n",
    "    u, v = P[n-1].astype('float64').idxmax()\n",
    "    state_seq = []\n",
    "    for j in range(n-1,0,-1):\n",
    "        t = B.loc[[(u, v)],j][0]\n",
    "        state_seq.append(v)\n",
    "        u, v = t, u\n",
    "    state_seq = state_seq[::-1][:-1] # Reverse and remove #STOP#\n",
    "    return state_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-NP', 'B-VP', 'B-SBAR', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'B-ADJP', 'I-ADJP', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "train_file = f\"../EN/train\"\n",
    "df_transition_matrix, all_state_pairs, states = train_transition(train_file)\n",
    "df_emission_matrix, observations= train_emission(train_file, states)\n",
    "\n",
    "\n",
    "sentence = \"He added that the stress-related compensation claims is about twice the average for all injury claims .\"\n",
    "sentence = sentence.split(' ')\n",
    "\n",
    "seq = second_order_viterbi(df_emission_matrix, observations, states, df_transition_matrix, all_state_pairs, sentence, is_preprocessed=False)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on EN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7e1e7c447649ed97b399106a0f2195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'EN'\n",
    "\n",
    "print(f\"Evaluating on {dataset}.\")\n",
    "\n",
    "in_file = f\"../{dataset}/dev.in\"\n",
    "train_file = f\"../{dataset}/train\"\n",
    "out_file = f\"../{dataset}/dev.p5.out\"\n",
    "\n",
    "# Train\n",
    "df_transition_matrix, all_state_pairs, states = train_transition(train_file)\n",
    "df_emission_matrix, observations= train_emission(train_file, states)\n",
    "\n",
    "# Create file handler to write to /dev.p5_fast.out\n",
    "outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "\n",
    "# Read in file\n",
    "with open(in_file, encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sent = [] # initialise array to store 1 sentence at a time.\n",
    "for word in tqdm(lines):\n",
    "\n",
    "    if word != \"\\n\":\n",
    "        sent.append(word.strip())\n",
    "\n",
    "    # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "    else:\n",
    "        # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "        sent_proc = preprocess_sentence(sent, observations)\n",
    "        # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "        sent_state_sequence = second_order_viterbi(df_emission_matrix, observations, states, df_transition_matrix, all_state_pairs, sent_proc, is_preprocessed=True)\n",
    "\n",
    "        for word, state in zip(sent, sent_state_sequence):\n",
    "            outf_h.write(word + ' ' + state)\n",
    "            outf_h.write(\"\\n\") # newline for each word\n",
    "        outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "        # Reset sentence list\n",
    "        sent = []\n",
    "\n",
    "outf_h.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running EvalScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiong\\Documents\\GitHub\\50.007-design-project\\EvalScript\n"
     ]
    }
   ],
   "source": [
    "%cd ../EvalScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n",
      "\n",
      "#Entity in gold data: 13179\n",
      "#Entity in prediction: 13371\n",
      "\n",
      "#Correct Entity : 10852\n",
      "Entity  precision: 0.8116\n",
      "Entity  recall: 0.8234\n",
      "Entity  F: 0.8175\n",
      "\n",
      "#Correct Sentiment : 10408\n",
      "Sentiment  precision: 0.7784\n",
      "Sentiment  recall: 0.7897\n",
      "Sentiment  F: 0.7840\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'EN'\n",
    "\n",
    "gold = f\"../{dataset}/dev.out\"\n",
    "pred = f\"../{dataset}/dev.p5.out\"\n",
    "print(dataset)\n",
    "!python evalResult.py $gold $pred\n",
    "print(\"=\" * 20, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a64fa4fae4734aaa7ba5951f77cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26807.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating on test.\")\n",
    "    \n",
    "in_file = f\"../EN/test.in\"\n",
    "train_file = f\"../EN/train\"\n",
    "out_file = f\"../EN/test.p5.out\"\n",
    "\n",
    "# Train\n",
    "df_transition_matrix, all_state_pairs, states = train_transition(train_file)\n",
    "df_emission_matrix, observations= train_emission(train_file, states)\n",
    "\n",
    "# Create file handler to write to /dev.p5_fast.out\n",
    "outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "\n",
    "# Read in file\n",
    "with open(in_file, encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sent = [] # initialise array to store 1 sentence at a time.\n",
    "for word in tqdm(lines):\n",
    "\n",
    "    if word != \"\\n\":\n",
    "        sent.append(word.strip())\n",
    "\n",
    "    # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "    else:\n",
    "        # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "        sent_proc = preprocess_sentence(sent, observations)\n",
    "        # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "        sent_state_sequence = second_order_viterbi(df_emission_matrix, observations, states, df_transition_matrix, all_state_pairs, sent_proc, is_preprocessed=True)\n",
    "\n",
    "        for word, state in zip(sent, sent_state_sequence):\n",
    "            outf_h.write(word + ' ' + state)\n",
    "            outf_h.write(\"\\n\") # newline for each word\n",
    "        outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "        # Reset sentence list\n",
    "        sent = []\n",
    "\n",
    "outf_h.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
