{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emission(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing emission parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state y, keep track of each observation count i.e. count (y -> x)\n",
    "    # before eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4}}\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # update emission_dict for state with count(y -> x) = 0\n",
    "    # after eg: {state1: {obs1: 1, obs2: 5}, state2: {obs1: 4, obs2: 0}}\n",
    "    observations = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "        \n",
    "        # process only valid lines\n",
    "        if len(split_line) == 2:\n",
    "            obs, state = split_line[0], split_line[1]\n",
    "            \n",
    "            observations.add(obs)\n",
    "            \n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "                \n",
    "            if obs not in emission_dict[state]:\n",
    "                emission_dict[state][obs] = 1\n",
    "            else:\n",
    "                emission_dict[state][obs] += 1\n",
    "\n",
    "    for k, v in emission_dict.items():\n",
    "        for obs in observations:\n",
    "            if obs not in v:\n",
    "                emission_dict[k][obs] = 0\n",
    "    \n",
    "    return emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_params_fixed(emission_dict, state, obs, k=0.5):\n",
    "    \n",
    "    if state not in emission_dict:\n",
    "        return 0\n",
    "    \n",
    "    state_data = emission_dict[state]\n",
    "    count_y = sum(state_data.values()) # count(y)\n",
    "    \n",
    "    if obs == \"#UNK#\":\n",
    "        count_y_to_x = k\n",
    "    else:\n",
    "        count_y_to_x = state_data[obs] # count(y -> x)\n",
    "    \n",
    "    return count_y_to_x / (count_y + k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transition(filename):\n",
    "    \"\"\"\n",
    "    Returns - a dictionary containing transition parameters\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # for each state u, keep track of each state count i.e. count (u,v)\n",
    "    # before eg: {START: {y1: 1, y2: 5}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, STOP: 3}}\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # after eg: {START: {y1: 1, y2: 5, STOP: 0}, y1: {y1: 3, y2: 4, STOP: 1}, y2: {y1: 1, y2: 0, STOP: 3}}\n",
    "    states = set()\n",
    "    states.add('STOP')\n",
    "    states.add('PREVSTART')\n",
    "    states.add('START')\n",
    "    \n",
    "    prev_prev_state = 'PREVSTART'\n",
    "    prev_state = 'START'\n",
    "        \n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "                \n",
    "        # Start new sequence\n",
    "        if len(split_line) < 2:\n",
    "            if (prev_prev_state, prev_state, 'STOP') not in transition_dict.keys():\n",
    "                transition_dict[(prev_prev_state, prev_state, 'STOP')] = 1\n",
    "            else:\n",
    "                transition_dict[(prev_prev_state, prev_state, 'STOP')] += 1\n",
    "            prev_prev_state = 'PREVSTART'\n",
    "            prev_state = 'START'\n",
    "\n",
    "        # processing the sentence\n",
    "        elif len(split_line) == 2:\n",
    "            curr_state = split_line[1]\n",
    "            states.add(curr_state)\n",
    "            \n",
    "            if (prev_prev_state, prev_state, curr_state) not in transition_dict.keys():\n",
    "                transition_dict[(prev_prev_state, prev_state, curr_state)] = 1\n",
    "            \n",
    "            else:\n",
    "                transition_dict[(prev_prev_state, prev_state, curr_state)] += 1\n",
    "            \n",
    "            prev_prev_state = prev_state\n",
    "            prev_state = curr_state\n",
    "            \n",
    "    \n",
    "    # Store all transition counts from state t to u\n",
    "    transition_count = {}\n",
    "    \n",
    "    for (t, u ,v), count in transition_dict.items():\n",
    "        if (t, u) not in transition_count.keys():\n",
    "            transition_count[(t, u)] = count\n",
    "        else:\n",
    "            transition_count[(t, u)] += count\n",
    "    \n",
    "    return transition_dict, transition_count, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_params(transition_dict, transition_count, t, u, v):\n",
    "    \n",
    "    if (t, u, v) not in transition_dict.keys():\n",
    "        return 0\n",
    "    \n",
    "    count_t_u_v = transition_dict[(t, u ,v)]\n",
    "    count_t_u = transition_count[(t, u)]\n",
    "            \n",
    "    return count_t_u_v / count_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480490669450607\n",
      "{('PREVSTART', 'START'): 7663, ('START', 'B-NP'): 4966, ('B-NP', 'I-NP'): 32390, ('I-NP', 'B-VP'): 7365, ('B-VP', 'B-ADVP'): 570, ('B-ADVP', 'B-ADJP'): 59, ('B-ADJP', 'I-ADJP'): 490, ('I-ADJP', 'I-ADJP'): 84, ('I-ADJP', 'B-PP'): 164, ('B-PP', 'B-NP'): 17064, ('I-NP', 'B-PP'): 8544, ('I-NP', 'O'): 12410, ('O', 'O'): 2710, ('O', 'B-ADJP'): 209, ('B-NP', 'B-VP'): 6164, ('B-VP', 'B-PP'): 1803, ('I-NP', 'I-NP'): 22201, ('B-VP', 'B-SBAR'): 467, ('B-SBAR', 'B-NP'): 1657, ('B-VP', 'B-NP'): 6304, ('B-VP', 'O'): 1231, ('O', 'B-NP'): 8288, ('I-NP', 'B-NP'): 2601, ('B-ADVP', 'B-PP'): 608, ('B-VP', 'I-VP'): 6828, ('I-VP', 'B-NP'): 3610, ('O', 'B-VP'): 2746, ('I-VP', 'I-VP'): 3331, ('B-NP', 'O'): 3830, ('B-NP', 'B-PP'): 2744, ('I-VP', 'B-PP'): 1507, ('B-PP', 'B-PP'): 340, ('B-NP', 'B-NP'): 1367, ('I-NP', 'B-SBAR'): 348, ('B-SBAR', 'B-VP'): 73, ('B-NP', 'B-ADVP'): 464, ('B-ADVP', 'B-NP'): 750, ('B-ADVP', 'I-ADVP'): 310, ('I-ADVP', 'B-NP'): 68, ('START', 'B-PP'): 833, ('I-VP', 'B-SBAR'): 134, ('I-VP', 'B-ADVP'): 382, ('B-ADVP', 'O'): 946, ('I-NP', 'B-ADVP'): 837, ('I-ADVP', 'B-ADVP'): 11, ('O', 'B-PP'): 1197, ('START', 'O'): 1087, ('I-VP', 'O'): 576, ('B-VP', 'B-PRT'): 204, ('B-PRT', 'B-NP'): 237, ('B-ADVP', 'B-VP'): 770, ('O', 'B-ADVP'): 697, ('B-PP', 'B-VP'): 489, ('B-NP', 'B-ADJP'): 152, ('B-ADJP', 'B-PP'): 428, ('I-ADVP', 'O'): 119, ('B-VP', 'B-VP'): 132, ('O', 'B-SBAR'): 385, ('I-ADJP', 'B-NP'): 51, ('B-NP', 'B-SBAR'): 161, ('B-VP', 'B-ADJP'): 716, ('B-ADJP', 'B-VP'): 194, ('I-NP', 'B-ADJP'): 224, ('B-ADJP', 'O'): 450, ('B-ADJP', 'B-NP'): 91, ('START', 'B-ADVP'): 416, ('B-PP', 'O'): 156, ('B-ADVP', 'B-SBAR'): 58, ('I-VP', 'B-VP'): 84, ('I-ADJP', 'O'): 184, ('START', 'B-ADJP'): 25, ('B-PP', 'I-PP'): 207, ('I-PP', 'B-NP'): 175, ('B-ADVP', 'B-ADVP'): 58, ('START', 'B-SBAR'): 173, ('I-PP', 'B-VP'): 7, ('I-ADJP', 'B-SBAR'): 35, ('B-ADJP', 'B-ADVP'): 28, ('B-PP', 'B-ADJP'): 48, ('B-PRT', 'B-PP'): 113, ('I-ADVP', 'B-PP'): 56, ('I-VP', 'B-ADJP'): 295, ('I-ADVP', 'I-ADVP'): 53, ('I-VP', 'B-PRT'): 237, ('B-ADJP', 'B-SBAR'): 66, ('START', 'B-CONJP'): 2, ('B-CONJP', 'I-CONJP'): 46, ('I-CONJP', 'B-NP'): 28, ('START', 'B-VP'): 143, ('B-SBAR', 'B-PP'): 27, ('B-NP', 'B-PRT'): 17, ('I-ADVP', 'B-SBAR'): 29, ('B-PRT', 'B-ADVP'): 14, ('B-PRT', 'O'): 74, ('O', 'B-INTJ'): 14, ('B-INTJ', 'I-INTJ'): 5, ('I-INTJ', 'O'): 5, ('I-PP', 'B-ADJP'): 1, ('B-INTJ', 'O'): 16, ('B-SBAR', 'B-SBAR'): 16, ('I-PP', 'B-PP'): 14, ('I-ADVP', 'B-VP'): 21, ('B-PP', 'B-ADVP'): 61, ('B-SBAR', 'O'): 54, ('B-SBAR', 'B-ADVP'): 17, ('B-INTJ', 'B-VP'): 2, ('I-PP', 'O'): 9, ('I-ADJP', 'B-VP'): 40, ('START', 'B-INTJ'): 10, ('B-SBAR', 'I-SBAR'): 48, ('I-SBAR', 'B-NP'): 46, ('B-PP', 'B-SBAR'): 18, ('I-NP', 'B-PRT'): 7, ('O', 'B-CONJP'): 25, ('I-CONJP', 'I-CONJP'): 18, ('I-CONJP', 'B-PP'): 7, ('I-ADJP', 'B-ADVP'): 8, ('B-PRT', 'B-VP'): 20, ('B-PRT', 'B-SBAR'): 9, ('I-PP', 'I-PP'): 16, ('B-VP', 'B-CONJP'): 3, ('I-CONJP', 'B-VP'): 10, ('B-SBAR', 'B-ADJP'): 6, ('I-SBAR', 'B-PP'): 1, ('B-PRT', 'B-ADJP'): 1, ('I-ADJP', 'B-ADJP'): 7, ('I-NP', 'B-CONJP'): 11, ('I-ADVP', 'B-ADJP'): 6, ('B-NP', 'B-UCP'): 1, ('B-UCP', 'I-UCP'): 1, ('I-UCP', 'I-UCP'): 3, ('I-UCP', 'B-NP'): 1, ('O', 'B-PRT'): 1, ('B-ADJP', 'B-PRT'): 1, ('START', 'B-LST'): 8, ('B-LST', 'O'): 11, ('B-ADJP', 'B-ADJP'): 2, ('B-ADVP', 'B-CONJP'): 2, ('B-CONJP', 'O'): 3, ('B-NP', 'B-CONJP'): 4, ('I-SBAR', 'B-VP'): 1, ('I-CONJP', 'O'): 1, ('I-PP', 'B-ADVP'): 1, ('I-VP', 'B-CONJP'): 2, ('I-INTJ', 'I-INTJ'): 2, ('B-SBAR', 'B-LST'): 1, ('O', 'B-LST'): 2, ('B-INTJ', 'B-NP'): 1, ('B-VP', 'B-INTJ'): 2, ('B-INTJ', 'B-ADVP'): 1, ('B-ADVP', 'B-PRT'): 1, ('B-INTJ', 'B-PP'): 1}\n"
     ]
    }
   ],
   "source": [
    "transition_dict, transition_count, states = train_transition('../dataset/EN/train')\n",
    "print(get_transition_params(transition_dict, transition_count, 'PREVSTART', 'START', 'B-NP'))\n",
    "print(transition_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_all_obs(emission_dict):\n",
    "    \"\"\"\n",
    "    Obtain all distinct observations words in the emission_dict.\n",
    "    Purpose: This helps us identify words in Test Set that do not exist in the Training Set (or the emission_dict)\n",
    "    Returns - Set of Strings.\n",
    "    \"\"\"\n",
    "    all_observations = set()\n",
    "    \n",
    "    for s_to_obs_dict in emission_dict.values():\n",
    "        for obs in s_to_obs_dict.keys():\n",
    "            all_observations.add(obs)\n",
    "            \n",
    "    return all_observations\n",
    "\n",
    "def preprocess_sentence(sentence, training_set_words):\n",
    "    \"\"\"\n",
    "    sentence - a list of Strings (words or observations)\n",
    "    Returns - a list of Strings, where Strings not in training_set_words are replaced by \"#UNK#\"\n",
    "    \"\"\"\n",
    "    return [ word if word in training_set_words else \"#UNK#\" for word in sentence ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_vertibi(emission_dict, transition_dict, transition_count, states, sentence, is_preprocessed):\n",
    "    # Helper functions\n",
    "    a = lambda t, u, v: get_transition_params(transition_dict, transition_count, t, u, v)\n",
    "    b = lambda state, obs: get_emission_params_fixed(emission_dict, state, obs, k=0.5)\n",
    "    \n",
    "    # Add all possible state combinations\n",
    "    all_states = set()\n",
    "    for u in states:\n",
    "        for v in states:\n",
    "            all_states.add((u, v))\n",
    "    \n",
    "    proc_sent = sentence\n",
    "    if not is_preprocessed:\n",
    "        training_set_words = obtain_all_obs(emission_dict)\n",
    "        proc_sent = preprocess_sentence(sentence, training_set_words)\n",
    "    \n",
    "    n = len(proc_sent) + 2\n",
    "    \n",
    "    # Pi Table\n",
    "#     P = np.zeros((len(all_states), n))\n",
    "    P = pd.DataFrame(index=all_states, columns=range(n)).fillna(0)\n",
    "    # Backtrace Table\n",
    "#     B = np.zeros((len(all_states), n))\n",
    "    B = pd.DataFrame(index=all_states, columns=range(n))\n",
    "    \n",
    "    # Initialise the starting states\n",
    "    P[0][('PREVSTART', 'START')] = 1\n",
    "#     P.loc[('PREVSTART', 'START'), 0] = 1\n",
    "    \n",
    "    # Recursive forward step\n",
    "    for j in range(1, n - 1):\n",
    "        x = proc_sent[j - 1]\n",
    "        \n",
    "        for v in states:  # Current state\n",
    "            for u in states:  # Previous state\n",
    "                for t in states: # Previous previous state\n",
    "                    p = P.loc[(t, u), j - 1] * a(t, u, v) * b(v, x)\n",
    "                    if p > P.loc[(u, v), j]:\n",
    "                        P.loc[(u, v), j] = p  # Update probability\n",
    "                        B.loc[(u, v), j] = t  # Update backpointer \n",
    "\n",
    "    \n",
    "    # Termination\n",
    "    j = n - 1\n",
    "    v = 'STOP'\n",
    "    for u in states: # Previous state\n",
    "        for t in states: # Previous previous stat\n",
    "            p = P.loc[(t, u), j - 1] * a(t, u, v)\n",
    "            if p > P.loc[(u, v), j]:\n",
    "                P.loc[(u, v), j] = p  # Update probability\n",
    "                B.loc[(u, v), j] = t  # Update backpointer\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Backtrace\n",
    "    state_combi = P[n-1].idxmax()\n",
    "\n",
    "    state_seq = []\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_state = B.loc[state_combi, i]\n",
    "        if isinstance(prev_state, str): # Check if previous state is of type str\n",
    "            state_seq.append(state_combi[1])\n",
    "            state_combi = (prev_state, state_combi[0])\n",
    "        else: # No possible transition to START\n",
    "            state_seq = ['O'] * n\n",
    "            break\n",
    "    state_seq = state_seq[::-1][:-1]  # reverse and drop STOP\n",
    "    \n",
    "    return P, B, state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../dataset/EN/train'\n",
    "emission_dict = train_emission(train_file)\n",
    "transition_dict, transition_count, states = train_transition(train_file)\n",
    "sentence = \"He added that the stress-related compensation claims is about twice the average for all injury claims .\"\n",
    "sentence = sentence.split(' ')\n",
    "\n",
    "_, _, seq = second_order_vertibi(emission_dict, transition_dict, transition_count, states, sentence, is_preprocessed=False)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ff7dd697964f44ae56845f112937ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on EN.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5379ca4c8c894405b9dd78a6c78eb902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-17e4e2ba7a8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0msent_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_state_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msecond_order_vertibi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_proc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_preprocessed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_state_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-7ea7a2dd1f7d>\u001b[0m in \u001b[0;36msecond_order_vertibi\u001b[1;34m(emission_dict, transition_dict, transition_count, states, sentence, is_preprocessed)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#                         P.loc[(u, v), j] = p  # Update probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#                         B.loc[(u, v), j] = t  # Update backpointer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                         \u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "sets = ['EN', 'SG', 'CN']\n",
    "\n",
    "for dataset in tqdm(sets):\n",
    "    \n",
    "    print(f\"Evaluating on {dataset}.\")\n",
    "    \n",
    "    in_file = f\"../dataset/{dataset}/dev.in\"\n",
    "    train_file = f\"../dataset/{dataset}/train\"\n",
    "    out_file = f\"../dataset/{dataset}/dev.p5.out\"\n",
    "    \n",
    "    # Train\n",
    "    emission_dict = train_emission(train_file)\n",
    "    transition_dict, transition_count, states = train_transition(train_file)\n",
    "    \n",
    "    # Obtain all distinct words in Training Set\n",
    "    training_set_words = obtain_all_obs(emission_dict)\n",
    "    \n",
    "    # Create file handler to write to /dev.p5.out\n",
    "    outf_h = open(out_file, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # Read in file\n",
    "    with open(in_file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sent = [] # initialise array to store 1 sentence at a time.\n",
    "    for word in tqdm(lines):\n",
    "        \n",
    "        if word != \"\\n\":\n",
    "            sent.append(word.strip())\n",
    "            \n",
    "        # We reached end of sentence - time to predict sentence's sequence of states (aka tags)\n",
    "        else:\n",
    "            # preprocess sentence (change unknown words to \"#UNK#\")\n",
    "            sent_proc = preprocess_sentence(sent, training_set_words)\n",
    "            # obtain processed sentence's predicted state seq (list of corresponding predicted states for each word in sent)\n",
    "            _, _, sent_state_sequence = second_order_vertibi(emission_dict, transition_dict, transition_count, states, sent_proc, is_preprocessed=True)\n",
    "            \n",
    "            for word, state in zip(sent, sent_state_sequence):\n",
    "                outf_h.write(word + ' ' + state)\n",
    "                outf_h.write(\"\\n\") # newline for each word\n",
    "            outf_h.write(\"\\n\") # another newline when end of sentence\n",
    "\n",
    "            # Reset sentence list\n",
    "            sent = []\n",
    "            \n",
    "    outf_h.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
